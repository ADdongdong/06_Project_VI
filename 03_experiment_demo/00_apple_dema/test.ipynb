{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_elbo(model, x, num_samples=10):\n",
    "    \"\"\"\n",
    "    计算未校正的哈密顿动力学模型下的ELBO\n",
    "    \n",
    "    参数：\n",
    "        model: 未校正的哈密顿动力学模型\n",
    "        x: 输入数据，大小为[batch_size, input_size]\n",
    "        num_samples: 采样数量\n",
    "    \n",
    "    返回：\n",
    "        elbo: ELBO（Evidence Lower Bound）\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, input_size = x.size()\n",
    "\n",
    "    # 从后验分布q(z|x)中采样num_samples个样本\n",
    "    z_samples = []\n",
    "    for i in range(num_samples):\n",
    "        z_q, _ = model.q_z(x)\n",
    "        z_samples.append(z_q)\n",
    "\n",
    "    # 将样本堆叠成张量\n",
    "    z_samples = torch.stack(z_samples)\n",
    "\n",
    "    # 计算解码器p(x|z)的对数似然\n",
    "    x_logits = model.p_x(z_samples).view(num_samples, batch_size, -1)\n",
    "    log_likelihood = F.log_softmax(x_logits, dim=-1).sum(-1).mean(0)\n",
    "\n",
    "    # 计算后验分布q(z|x)与先验分布p(z)之间的KL散度\n",
    "    z_p = model.sample_prior(num_samples)\n",
    "    kl_divergence = torch.distributions.kl_divergence(\n",
    "        torch.distributions.Normal(z_samples.mean(0), z_samples.std(0)),\n",
    "        torch.distributions.Normal(z_p, torch.ones_like(z_p))\n",
    "    ).sum(-1).mean(0)\n",
    "\n",
    "    # 计算ELBO\n",
    "    elbo = log_likelihood - kl_divergence\n",
    "\n",
    "    return -elbo  # 返回负数，因为我们使用优化器最小化损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-1.6549392, -0.7325643], dtype=float32), array([-0.9294475, -0.6207985], dtype=float32), array([-1.3915583,  1.9626707], dtype=float32), array([ 0.25071254, -0.6399836 ], dtype=float32), array([-0.07853568, -1.9527625 ], dtype=float32), array([-0.7784954,  1.0815153], dtype=float32), array([-0.31023294,  0.45576578], dtype=float32), array([1.3170817 , 0.06579784], dtype=float32), array([ 0.9772804, -0.0321852], dtype=float32), array([ 0.68810326, -0.62953514], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from jax import grad\n",
    "\n",
    "class UHA:\n",
    "    '''\n",
    "    这段代码实现了HMC和UHA的步骤。HMC是一种基于哈密顿动力学的MCMC方法，\n",
    "    它通过在动量空间中进行随机游走来探索目标分布。\n",
    "    在每个时间步长内，它使用梯度信息来更新动量和位置变量。\n",
    "    具体而言，它首先对动量进行半个时间步长的更新，\n",
    "    然后对位置进行一个完整的时间步长的更新，\n",
    "    最后再对动量进行半个时间步长的更新。\n",
    "\n",
    "    UHA是一种基于未校正哈密顿动力学的MCMC方法，它与HMC类似，\n",
    "    但使用未校正哈密顿动力学方程来生成样本。\n",
    "    在UHA中，我们从标准正态分布中抽取一个随机初始状态，\n",
    "    并使用未校正哈密顿动力学方程进行L_m次采样。\n",
    "    每次采样都会生成一个新状态，并将其用于计算ELBO下界。\n",
    "    '''\n",
    "    def __init__(self, dim, L_m=10, eps_m=0.1):\n",
    "        self.dim = dim\n",
    "        self.L_m = L_m\n",
    "        self.eps_m = eps_m\n",
    "\n",
    "    #定义能量函数E\n",
    "    def E(self, x):\n",
    "        return torch.sum(x ** 2) / 2\n",
    "\n",
    "    #定义能量函数的梯度\n",
    "    def grad_E(self, x):\n",
    "        E = self.E(x)\n",
    "        # dE/dx\n",
    "        return torch.autograd.grad(E, x)[0]\n",
    "\n",
    "    #HMC步骤\n",
    "    def HMC_step(self, x, p, eps):\n",
    "        # 按照哈密顿动力学仿鲿进行一半时间不长的动量更新\n",
    "        p = p - eps * self.grad_E(x) / 2\n",
    "        # 按照哈密顿动力学仿鲿进行一整个时间步长的坐标更新\n",
    "        x = x + eps * p\n",
    "        # 按照哈密顿动力学方程进行一班时间步长的动量更新\n",
    "        p = p - eps * self.grad_E(x) / 2\n",
    "        return x, p\n",
    "    \n",
    "    #UHA步骤，未校正哈密顿\n",
    "    def UHA_step(self, x):\n",
    "        #从标准正态分布中抽样\n",
    "        p = torch.randn_like(x)\n",
    "        for i in range(self.L_m):\n",
    "            #使用HMC步骤进行L_m次采样，并更新x和p\n",
    "            x, p = self.HMC_step(x, p, self.eps_m)\n",
    "        return x\n",
    "\n",
    "    #采样函数\n",
    "    def sample(self, num_samples=1):\n",
    "        samples = []\n",
    "        for i in range(num_samples):\n",
    "            # 初始化当前状态\n",
    "            x = torch.randn(self.dim).requires_grad_(True)\n",
    "\n",
    "            # 运行未校正哈密顿算法\n",
    "            for m in range(1000):\n",
    "                # 从未校正的状态转移矩阵中采样(从转移核中采样)\n",
    "                y = self.UHA_step(x)\n",
    "\n",
    "            # 保存在理目标分布的最终样本(在UHA中没有使用)\n",
    "            samples.append(y.detach().numpy())\n",
    "        return samples\n",
    "\n",
    "    #定义函数从q(z|x)中采样\n",
    "    def q_z(self, x:list, func)->list:\n",
    "        '''\n",
    "            参数x: 要采样的样本个数\n",
    "            参数func: 对那个分布(函数)进行采样\n",
    "            返回值：采样成功以后的样本列表\n",
    "        '''\n",
    "        mu, logvar = encoder(x)\n",
    "        #传入进来的是空列表\n",
    "        sample = self.sample(x)\n",
    "        return sample\n",
    "\n",
    "    #定义函数丛p(z|x)中采样\n",
    "    def p_z(x:list)->list:\n",
    "        '''\n",
    "            函数返回值为采样的列表\n",
    "        '''\n",
    "\n",
    "uha = UHA(2)\n",
    "sample = uha.sample(10)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对z进行哈密顿采样\n",
    "import torch\n",
    "\n",
    "# Define the target function\n",
    "def target_function(mu, logvar):\n",
    "    # Compute the log probability of a normal distribution with mean mu and variance exp(logvar)\n",
    "    return -0.5 * (mu**2 + torch.exp(logvar) - logvar - 1)\n",
    "\n",
    "# Define the Hamiltonian dynamics\n",
    "def hamiltonian_dynamics(mu, logvar):\n",
    "    # Compute the gradients of the target function with respect to mu and logvar\n",
    "    grad_mu, grad_logvar = torch.autograd.grad(target_function(mu, logvar), [mu, logvar])\n",
    "\n",
    "    # Return the gradients as a tuple\n",
    "    return grad_mu, grad_logvar\n",
    "\n",
    "# Define the Hamiltonian Monte Carlo sampler\n",
    "def hmc_sampler(mu_init, logvar_init, num_steps, step_size):\n",
    "    # Initialize the current state\n",
    "    mu_current = mu_init\n",
    "    logvar_current = logvar_init\n",
    "\n",
    "    # Initialize the current momentum (sampled from a standard normal distribution)\n",
    "    momentum_current = torch.randn_like(mu_init)\n",
    "\n",
    "    # Initialize the current Hamiltonian (target function plus kinetic energy)\n",
    "    hamiltonian_current = target_function(mu_current, logvar_current) + 0.5 *torch.sum(momentum_current**2)\n",
    "\n",
    "    # Perform the leapfrog integration\n",
    "    for i in range(num_steps):\n",
    "        # Compute the gradients of the Hamiltonian with respect to mu and logvar\n",
    "        grad_mu, grad_logvar = hamiltonian_dynamics(mu_current, logvar_current)\n",
    "\n",
    "        # Update the momentum\n",
    "        momentum_current -= 0.5 * step_size * grad_logvar\n",
    "        mu_current += step_size * momentum_current\n",
    "        logvar_current += step_size * grad_mu\n",
    "        momentum_current -= 0.5 * step_size * grad_logvar\n",
    "\n",
    "    # Compute the new Hamiltonian (target function plus kinetic energy)\n",
    "    hamiltonian_new = target_function(mu_current, logvar_current) + 0.5 * torch.sum(momentum_current**2)\n",
    "\n",
    "    # Accept or reject the new state based on the Metropolis-Hastings criterion\n",
    "    if torch.rand(1) < torch.exp(hamiltonian_current - hamiltonian_new):\n",
    "        return mu_current, logvar_current\n",
    "    else:\n",
    "        return mu_init, logvar_init\n",
    "\n",
    "# Example usage:\n",
    "mu_init = torch.zeros(10)\n",
    "logvar_init = torch.zeros(10)\n",
    "num_steps = 10\n",
    "step_size = 0.1\n",
    "\n",
    "mu_sampled, logvar_sampled = hmc_sampler(mu_init, logvar_init, num_steps, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m num_steps \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m     53\u001b[0m step_size \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[0;32m---> 55\u001b[0m mu_sampled, logvar_sampled \u001b[39m=\u001b[39m hmc_sampler(mu_init, logvar_init, num_steps, step_size)\n",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m, in \u001b[0;36mhmc_sampler\u001b[0;34m(mu_init, logvar_init, num_steps, step_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# Perform the leapfrog integration\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_steps):\n\u001b[1;32m     31\u001b[0m     \u001b[39m# Compute the gradients of the Hamiltonian with respect to mu and logvar\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     grad_mu, grad_logvar \u001b[39m=\u001b[39m hamiltonian_dynamics(mu_current, logvar_current)\n\u001b[1;32m     34\u001b[0m     \u001b[39m# Update the momentum\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     momentum_current \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m step_size \u001b[39m*\u001b[39m grad_logvar\n",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m, in \u001b[0;36mhamiltonian_dynamics\u001b[0;34m(mu, logvar)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhamiltonian_dynamics\u001b[39m(mu, logvar):\n\u001b[1;32m     11\u001b[0m     \u001b[39m# Compute the gradients of the target function with respect to mu and logvar\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     grad_mu, grad_logvar \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(target_function(mu, logvar), [mu, logvar])\n\u001b[1;32m     14\u001b[0m     \u001b[39m# Return the gradients as a tuple\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_mu, grad_logvar\n",
      "File \u001b[0;32m~/miniconda3/envs/numpyro/lib/python3.9/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "#对z进行未校正的哈密顿采样\n",
    "import torch\n",
    "\n",
    "# Define the target function\n",
    "def target_function(mu, logvar):\n",
    "    # Compute the log probability of a normal distribution with mean mu and variance exp(logvar)\n",
    "    return -0.5 * (mu**2 + torch.exp(logvar) - logvar - 1)\n",
    "\n",
    "# Define the Hamiltonian dynamics\n",
    "def hamiltonian_dynamics(mu, logvar):\n",
    "    # Compute the gradients of the target function with respect to mu and logvar\n",
    "    grad_mu, grad_logvar = torch.autograd.grad(target_function(mu, logvar), [mu, logvar])\n",
    "\n",
    "    # Return the gradients as a tuple\n",
    "    return grad_mu, grad_logvar\n",
    "\n",
    "# Define the Hamiltonian Monte Carlo sampler\n",
    "def hmc_sampler(mu_init, logvar_init, num_steps, step_size):\n",
    "    # Initialize the current state\n",
    "    mu_current = mu_init\n",
    "    logvar_current = logvar_init\n",
    "\n",
    "    # Initialize the current momentum (sampled from a standard normal distribution)\n",
    "    momentum_current = torch.randn_like(mu_init)\n",
    "\n",
    "    # Initialize the current Hamiltonian (target function plus kinetic energy)\n",
    "    hamiltonian_current = target_function(mu_current, logvar_current) + 0.5 *torch.sum(momentum_current**2)\n",
    "\n",
    "    # Perform the leapfrog integration\n",
    "    for i in range(num_steps):\n",
    "        # Compute the gradients of the Hamiltonian with respect to mu and logvar\n",
    "        grad_mu, grad_logvar = hamiltonian_dynamics(mu_current, logvar_current)\n",
    "\n",
    "        # Update the momentum\n",
    "        momentum_current -= 0.5 * step_size * grad_logvar\n",
    "        mu_current += step_size * momentum_current\n",
    "        logvar_current += step_size * grad_mu\n",
    "        momentum_current -= 0.5 * step_size * grad_logvar\n",
    "\n",
    "    # Compute the new Hamiltonian (target function plus kinetic energy)\n",
    "    hamiltonian_new = target_function(mu_current, logvar_current) + 0.5 * torch.sum(momentum_current**2)\n",
    "\n",
    "    # Accept or reject the new state based on the Metropolis-Hastings criterion\n",
    "    if torch.rand(1) < torch.exp(hamiltonian_current - hamiltonian_new):\n",
    "        return mu_current, logvar_current\n",
    "    else:\n",
    "        return mu_init, logvar_init\n",
    "\n",
    "# Example usage:\n",
    "mu_init = torch.zeros(10)\n",
    "logvar_init = torch.zeros(10)\n",
    "num_steps = 10\n",
    "step_size = 0.1\n",
    "\n",
    "mu_sampled, logvar_sampled = hmc_sampler(mu_init, logvar_init, num_steps, step_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Mar  8 2023, 14:00:05) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2af601dfafe7974b0cebf1f2b52985470342a39253c20dfe2870dd208a17d3da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
