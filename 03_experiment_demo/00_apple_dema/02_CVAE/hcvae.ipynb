{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import utils\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HCVAE(nn.Module):\n",
    "    \"\"\"Implementation of CVAE(Conditional Variational Auto-Encoder)\"\"\"\n",
    "\n",
    "    def __init__(self, feature_size, class_size, latent_size):\n",
    "        super(HCVAE, self).__init__()\n",
    "\n",
    "        # 定义均方差对象\n",
    "        self.Loss_MSE = torch.nn.MSELoss()\n",
    "\n",
    "        # 定义网络\n",
    "        self.fc2_mu = nn.Linear(200, latent_size)\n",
    "        self.fc2_log_std = nn.Linear(200, latent_size)\n",
    "        self.fc1_mu = nn.Linear(200, feature_size)\n",
    "        self.fc1_log_std = nn.Linear(200, feature_size)\n",
    "        # 编码\n",
    "        self.encoder_fc1 = nn.Linear(feature_size + class_size, 200)\n",
    "        self.encoder_fc2 = nn.Linear(feature_size + class_size, 200)\n",
    "        self.encoder_fc3 = nn.Linear(feature_size + class_size, 200)\n",
    "\n",
    "        # 解码\n",
    "        self.decoder_fc1 = nn.Linear(latent_size + class_size, 200)\n",
    "        self.decoder_fc2 = nn.Linear(feature_size + class_size, 200)\n",
    "        self.decoder_fc3 = nn.Linear(feature_size + class_size, 200)\n",
    "        self.decoder_mu = nn.Linear(200, feature_size)\n",
    "        self.decoder_log_std = nn.Linear(200, feature_size)\n",
    "\n",
    "    def encode1_2(self, func, x, y):\n",
    "        # concat features and labels\n",
    "        h1 = F.relu(func(torch.cat([x, y], dim=1)))\n",
    "        mu = self.fc1_mu(h1)\n",
    "        log_std = self.fc1_log_std(h1)\n",
    "        return mu, log_std\n",
    "\n",
    "    def encode3(self, x, y):\n",
    "        h1 = F.relu(self.encoder_fc3(torch.cat([x, y], dim=1)))\n",
    "        mu = self.fc2_mu(h1)\n",
    "        log_std = self.fc2_log_std(h1)\n",
    "        return mu, log_std\n",
    "\n",
    "    def decode1(self, z, y):\n",
    "        # concat latents and labels\n",
    "        h3 = F.relu(self.decoder_fc1(torch.cat([z, y], dim=1)))\n",
    "        # 这里decoder也是先decoder出来均值和方差，因为，后面计算loss函数要用\n",
    "        # 在decoder后再使用reparametrize重采样出来一个z放入下一层解码\n",
    "        de_mu = self.fc1_mu(h3)\n",
    "        de_log_std = self.fc1_log_std(h3)\n",
    "\n",
    "        return de_mu, de_log_std\n",
    "        \n",
    "\n",
    "    def decode2_3(self, z, y):\n",
    "        # concat latents and labels\n",
    "        h3 = F.relu(self.decode2_3(torch.cat([z, y], dim=1)))\n",
    "        # 这里decoder也是先decoder出来均值和方差，因为，后面计算loss函数要用\n",
    "        # 在decoder后再使用reparametrize重采样出来一个z放入下一层解码\n",
    "        de_mu = self.fc1_mu(h3)\n",
    "        de_log_std = self.fc1_log_std(h3)\n",
    "        return de_mu, de_log_std\n",
    "\n",
    "    def final_decode(self, z, y):\n",
    "        h3 = F.relu(self.decoder_fc3(torch.cat([z, y], dim=1)))\n",
    "        mu = self.decoder_mu(h3)\n",
    "        log_std = self.decoder_log_std(h3)\n",
    "        return mu, log_std\n",
    "\n",
    "    def reparametrize(self, mu, log_std):\n",
    "        std = torch.exp(log_std)\n",
    "        eps = torch.randn_like(std)  # simple from standard normal distribution\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # 第一次条件编码\n",
    "        mu_1, log_std_1 = self.encode1_2(self.encoder_fc1, x, y)\n",
    "        z_1 = self.reparametrize(mu_1, log_std_1)\n",
    "\n",
    "        # 第二次条件编码\n",
    "        mu_2, log_std_2 = self.encode1_2(self.encoder_fc2, z_1, y)\n",
    "        z2 = self.reparametrize(mu_2, log_std_2)\n",
    "\n",
    "        # 第三次条件编码\n",
    "        mu_3, log_std_3 = self.encode3(z2, y)\n",
    "        z3 = self.reparametrize(mu_3, log_std_3)\n",
    "\n",
    "        # 第一次条件解码\n",
    "        de_mu1, de_log1 = self.decode1(z3, y)\n",
    "        de_z1 = self.reparametrize(de_mu1, de_log1)\n",
    "\n",
    "        # 第二次条件解码\n",
    "        de_mu2, de_log2 = self.decode2_3(de_z1, y)\n",
    "        de_z2 = self.reparametrize(de_mu2, de_log2)\n",
    "\n",
    "        # 第三次条件解码\n",
    "        de_mu3, de_log3 = self.final_decode(de_z2, y)\n",
    "        de_z3 = self.reparametrize(de_mu3, de_log3)\n",
    "\n",
    "        # 根据计算loss函数所用到的内容\n",
    "        # 将编码得到的方差打包成数组\n",
    "        en_log = [log_std_1, log_std_2, log_std_3]\n",
    "        # 将编码和解码得到的均值打包成方差\n",
    "        en_mu = [mu_1, mu_2, mu_3]\n",
    "        de_mu = [de_mu1, de_mu2, de_mu3]\n",
    "\n",
    "        loss = self.loss_function(en_log, en_mu, de_mu)\n",
    "        return loss\n",
    "\n",
    "    def loss_function(self, en_log, en_mu, de_mu) -> torch.Tensor:\n",
    "        # 根据hcvae的loss公式来计算loss函数\n",
    "        # 方差部分为编码器的方差之和\n",
    "        logvar_sum = -torch.sum(torch.log(en_log[0]))\n",
    "        logvar_sum = logvar_sum - torch.sum(torch.log(en_log[1]))\n",
    "        logvar_sum = logvar_sum - torch.sum(torch.log(en_log[2]))\n",
    "        print('logvar_sum',logvar_sum)\n",
    "\n",
    "        #  均值部分为编码器和解码器d对应的均值的均方差\n",
    "        mu_sum = self.Loss_MSE(en_mu[0], de_mu[1])\n",
    "        mu_sum = mu_sum +  self.Loss_MSE(en_mu[1], de_mu[0])\n",
    "        print('mu_sum', mu_sum)\n",
    "\n",
    "        # 计算整体的loss函数\n",
    "        loss = logvar_sum + mu_sum\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decode2_3() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39m# save val model\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     utils\u001b[39m.\u001b[39msave_model(cvae, \u001b[39m\"\u001b[39m\u001b[39m./model_weights/cvae/cvae_weights.pth\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m test2()\n",
      "Cell \u001b[1;32mIn[23], line 33\u001b[0m, in \u001b[0;36mtest2\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m y \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mto_one_hot(label\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), num_class\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39m#recon, mu, log_std = cvae(inputs, y)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m#loss = cvae.loss_function(recon, inputs, mu, log_std)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m loss \u001b[39m=\u001b[39m cvae\u001b[39m.\u001b[39;49mforward(inputs, y)\n\u001b[0;32m     34\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     35\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[22], line 90\u001b[0m, in \u001b[0;36mHCVAE.forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     87\u001b[0m de_z1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparametrize(de_mu1, de_log1)\n\u001b[0;32m     89\u001b[0m \u001b[39m# 第二次条件解码\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m de_mu2, de_log2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode2_3(de_z1, y)\n\u001b[0;32m     91\u001b[0m de_z2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparametrize(de_mu2, de_log2)\n\u001b[0;32m     93\u001b[0m \u001b[39m# 第三次条件解码\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 53\u001b[0m, in \u001b[0;36mHCVAE.decode2_3\u001b[1;34m(self, z, y)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode2_3\u001b[39m(\u001b[39mself\u001b[39m, z, y):\n\u001b[0;32m     52\u001b[0m     \u001b[39m# concat latents and labels\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     h3 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode2_3(torch\u001b[39m.\u001b[39;49mcat([z, y], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)))\n\u001b[0;32m     54\u001b[0m     \u001b[39m# 这里decoder也是先decoder出来均值和方差，因为，后面计算loss函数要用\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[39m# 在decoder后再使用reparametrize重采样出来一个z放入下一层解码\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     de_mu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1_mu(h3)\n",
      "\u001b[1;31mTypeError\u001b[0m: decode2_3() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "def test2():\n",
    "    epochs = 100\n",
    "    batch_size = 100\n",
    "\n",
    "    recon = None\n",
    "    img = None\n",
    "\n",
    "    utils.make_dir(\"./img/cvae\")\n",
    "    utils.make_dir(\"./model_weights/cvae\")\n",
    "\n",
    "    train_data = torchvision.datasets.MNIST(\n",
    "        root='./mnist',\n",
    "        train=True,\n",
    "        transform=torchvision.transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    data_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "    cvae = HCVAE(feature_size=784, class_size=10, latent_size=10)\n",
    "\n",
    "    optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        train_loss = 0\n",
    "        i = 0\n",
    "        for batch_id, data in enumerate(data_loader):\n",
    "            img, label = data\n",
    "            inputs = img.reshape(img.shape[0], -1)\n",
    "            y = utils.to_one_hot(label.reshape(-1, 1), num_class=10)\n",
    "            #recon, mu, log_std = cvae(inputs, y)\n",
    "            #loss = cvae.loss_function(recon, inputs, mu, log_std)\n",
    "            loss = cvae.forward(inputs, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            i += 1\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"Epoch[{}/{}], Batch[{}/{}], batch_loss:{:.6f}\".format(\n",
    "                    epoch+1, epochs, batch_id+1, len(data_loader), loss.item()))\n",
    "\n",
    "        print(\"======>epoch:{},\\t epoch_average_batch_loss:{:.6f}============\".format(\n",
    "            epoch+1, train_loss/i), \"\\n\")\n",
    "\n",
    "        # save imgs\n",
    "        if epoch % 10 == 0:\n",
    "            # 查看图像\n",
    "            imgs = utils.to_img(recon.detach())\n",
    "            path = \"./img/cvae/epoch{}.png\".format(epoch+1)\n",
    "            torchvision.utils.save_image(imgs, path, nrow=10)\n",
    "            print(\"save:\", path, \"\\n\")\n",
    "\n",
    "    torchvision.utils.save_image(img, \"./img/cvae/raw.png\", nrow=10)\n",
    "    print(\"save raw image:./img/cvae/raw/png\", \"\\n\")\n",
    "\n",
    "    # save val model\n",
    "    utils.save_model(cvae, \"./model_weights/cvae/cvae_weights.pth\")\n",
    "\n",
    "test2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "240bc028caeb8b02ff80d8aedfc61caf7a0e4db2770780d40c5b717508bae340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
