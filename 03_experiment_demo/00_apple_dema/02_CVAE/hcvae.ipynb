{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import utils\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个HCVAE是第一版\n",
    "# 在解码的时候，先计算出来均值和方差，然后再使用重参数化得到其重构误差\n",
    "class HCVAE(nn.Module):\n",
    "    \"\"\"Implementation of CVAE(Conditional Variational Auto-Encoder)\"\"\"\n",
    "\n",
    "    def __init__(self, feature_size, class_size, latent_size):\n",
    "        super(HCVAE, self).__init__()\n",
    "\n",
    "        # 定义均方差对象\n",
    "        self.Loss_MSE = torch.nn.MSELoss()\n",
    "\n",
    "        '''\n",
    "            在这个网络中，对与每一层编码和解码。\n",
    "            都是一个两层的网络结构；先将数据转换为200维度的数据，\n",
    "            然后，将200维度的数据再转换为重构值或者均值方差\n",
    "        '''\n",
    "        # 定义网络\n",
    "        self.fc2_mu = nn.Linear(200, latent_size)\n",
    "        self.fc2_log_std = nn.Linear(200, latent_size)\n",
    "        self.fc1_mu = nn.Linear(200, feature_size)\n",
    "        self.fc1_log_std = nn.Linear(200, feature_size)\n",
    "\n",
    "        # 编码\n",
    "        self.encoder_fc1 = nn.Linear(feature_size + class_size, 200)\n",
    "        self.encoder_fc2 = nn.Linear(feature_size + class_size, 200)\n",
    "        self.encoder_fc3 = nn.Linear(feature_size + class_size, 200)\n",
    "\n",
    "        # 解码\n",
    "        self.decoder_fc1 = nn.Linear(latent_size + class_size, 200)\n",
    "        self.decoder_fc2 = nn.Linear(feature_size + class_size, 200)\n",
    "        self.decoder_fc3 = nn.Linear(feature_size + class_size, 200)\n",
    "        self.decoder_mu = nn.Linear(200, feature_size)\n",
    "        self.decoder_log_std = nn.Linear(200, feature_size)\n",
    "\n",
    "    def encode1_2(self, func, x, y):\n",
    "        # concat features and labels\n",
    "        h1 = F.relu(func(torch.cat([x, y], dim=1)))\n",
    "        mu = self.fc1_mu(h1)\n",
    "        log_std = self.fc1_log_std(h1)\n",
    "        return mu, log_std\n",
    "\n",
    "    def encode3(self, x, y):\n",
    "        h1 = F.relu(self.encoder_fc3(torch.cat([x, y], dim=1)))\n",
    "        mu = self.fc2_mu(h1)\n",
    "        log_std = self.fc2_log_std(h1)\n",
    "        return mu, log_std\n",
    "\n",
    "    def decode1(self, z, y):\n",
    "        # concat latents and labels\n",
    "        h3 = F.relu(self.decoder_fc1(torch.cat([z, y], dim=1)))\n",
    "        # 这里decoder也是先decoder出来均值和方差，因为，后面计算loss函数要用\n",
    "        # 在decoder后再使用reparametrize重采样出来一个z放入下一层解码\n",
    "        de_mu = self.fc1_mu(h3)\n",
    "        de_log_std = self.fc1_log_std(h3)\n",
    "\n",
    "        return de_mu, de_log_std\n",
    "        \n",
    "\n",
    "    def decode2_3(self, z, y):\n",
    "        # concat latents and labels\n",
    "        h3 = F.relu(self.decoder_fc3(torch.cat([z, y], dim=1)))\n",
    "        # 这里decoder也是先decoder出来均值和方差，因为，后面计算loss函数要用\n",
    "        # 在decoder后再使用reparametrize重采样出来一个z放入下一层解码\n",
    "        de_mu = self.fc1_mu(h3)\n",
    "        de_log_std = self.fc1_log_std(h3)\n",
    "        return de_mu, de_log_std\n",
    "\n",
    "    def final_decode(self, z, y):\n",
    "        h3 = F.relu(self.decoder_fc3(torch.cat([z, y], dim=1)))\n",
    "        mu = self.decoder_mu(h3)\n",
    "        log_std = self.decoder_log_std(h3)\n",
    "        return mu, log_std\n",
    "\n",
    "    def reparametrize(self, mu, log_std):\n",
    "        std = torch.exp(log_std)\n",
    "        eps = torch.randn_like(std)  # simple from standard normal distribution\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # 第一次条件编码\n",
    "        mu_1, log_std_1 = self.encode1_2(self.encoder_fc1, x, y)\n",
    "        z_1 = self.reparametrize(mu_1, log_std_1)\n",
    "\n",
    "        # 第二次条件编码\n",
    "        mu_2, log_std_2 = self.encode1_2(self.encoder_fc2, z_1, y)\n",
    "        z2 = self.reparametrize(mu_2, log_std_2)\n",
    "\n",
    "        # 第三次条件编码\n",
    "        mu_3, log_std_3 = self.encode3(z2, y)\n",
    "        z3 = self.reparametrize(mu_3, log_std_3)\n",
    "\n",
    "        # 第一次条件解码\n",
    "        # 先解码出重构值，再根据重构值计算其均值和方差\n",
    "        de_mu3, de_log3 = self.decode1(z3, y)\n",
    "        recon3 = self.reparametrize(de_mu3, de_log3)\n",
    "\n",
    "        # 第二次条件解码\n",
    "        de_mu2, de_log2 = self.decode2_3(recon3, y)\n",
    "        recon2 = self.reparametrize(de_mu2, de_log2)\n",
    "\n",
    "        # 第三次条件解码\n",
    "        de_mu1, de_log1 = self.final_decode(recon2, y)\n",
    "        de_z3 = self.reparametrize(de_mu1, de_log1)\n",
    "\n",
    "        # 根据计算loss函数所用到的内容\n",
    "        # 将编码得到的方差打包成数组\n",
    "        en_log = [log_std_1, log_std_2, log_std_3]\n",
    "        # 将编码和解码得到的均值打包成方差\n",
    "        en_mu = [mu_1, mu_2, mu_3]\n",
    "        de_mu = [de_mu1, de_mu2, de_mu3]\n",
    "\n",
    "        loss = self.loss_function(en_log, en_mu, de_mu)\n",
    "        return loss\n",
    "\n",
    "    def loss_function(self, en_log, en_mu, de_mu) -> torch.Tensor:\n",
    "        # 根据hcvae的loss公式来计算loss函数\n",
    "        # 方差部分为编码器的方差之和\n",
    "        logvar_sum = -torch.sum(torch.log(en_log[0]))\n",
    "        logvar_sum = logvar_sum - torch.sum(torch.log(en_log[1]))\n",
    "        logvar_sum = logvar_sum - torch.sum(torch.log(en_log[2]))\n",
    "        #print('logvar_sum',logvar_sum)\n",
    "\n",
    "        #  均值部分为编码器和解码器d对应的均值的均方差\n",
    "        mu_sum = self.Loss_MSE(en_mu[0], de_mu[1])\n",
    "        mu_sum = mu_sum +  self.Loss_MSE(en_mu[1], de_mu[0])\n",
    "        #print('mu_sum', mu_sum)\n",
    "\n",
    "        # 修改loss函数的计算方式，改成kl散度+重构误差\n",
    "\n",
    "        # 计算整体的loss函数\n",
    "        loss = logvar_sum + mu_sum\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "这是第二版HCVAE，第一版的问题是，loss会变成nan值\n",
    "第二版改变了计算loss函数的方式，不用HVAE的均值和方差计算\n",
    "而是使用传统的kl散度和重构误差之和作为loss函数值看看效果\n",
    "'''\n",
    "class HCVAE2(nn.Module):\n",
    "    \"\"\"Implementation of CVAE(Conditional Variational Auto-Encoder)\"\"\"\n",
    "\n",
    "    def __init__(self, feature_size, class_size, latent_size):\n",
    "        super(HCVAE2, self).__init__()\n",
    "\n",
    "        # 定义均方差对象\n",
    "        self.Loss_MSE = torch.nn.MSELoss()\n",
    "\n",
    "        # 定义网络\n",
    "        self.fc2_mu = nn.Linear(200, latent_size)\n",
    "        self.fc2_log_std = nn.Linear(200, latent_size)\n",
    "        self.fc1_mu = nn.Linear(200, feature_size)\n",
    "        self.fc1_log_std = nn.Linear(200, feature_size)\n",
    "        # 编码\n",
    "        self.encoder_fc1 = nn.Linear(feature_size + class_size, 200)\n",
    "        self.encoder_fc2 = nn.Linear(feature_size + class_size, 200)\n",
    "        self.encoder_fc3 = nn.Linear(feature_size + class_size, 200)\n",
    "\n",
    "        # 解码\n",
    "        self.decoder_fc1 = nn.Linear(latent_size + class_size, 200)\n",
    "        self.decoder_fc2 = nn.Linear(feature_size + class_size, 200)\n",
    "        self.decoder_fc3 = nn.Linear(feature_size + class_size, 200)\n",
    "        self.decoder_mu = nn.Linear(200, feature_size)\n",
    "        self.decoder_log_std = nn.Linear(200, feature_size)\n",
    "\n",
    "    def encode1_2(self, func, x, y):\n",
    "        # concat features and labels\n",
    "        h1 = F.relu(func(torch.cat([x, y], dim=1)))\n",
    "        mu = self.fc1_mu(h1)\n",
    "        log_std = self.fc1_log_std(h1)\n",
    "        return mu, log_std\n",
    "\n",
    "    def encode3(self, x, y):\n",
    "        h1 = F.relu(self.encoder_fc3(torch.cat([x, y], dim=1)))\n",
    "        mu = self.fc2_mu(h1)\n",
    "        log_std = self.fc2_log_std(h1)\n",
    "        return mu, log_std\n",
    "\n",
    "    def decode1(self, z, y):\n",
    "        # concat latents and labels\n",
    "        h3 = F.relu(self.decoder_fc1(torch.cat([z, y], dim=1)))\n",
    "        # 这里decoder也是先decoder出来均值和方差，因为，后面计算loss函数要用\n",
    "        # 在decoder后再使用reparametrize重采样出来一个z放入下一层解码\n",
    "        de_mu = self.fc1_mu(h3)\n",
    "        de_log_std = self.fc1_log_std(h3)\n",
    "\n",
    "        return de_mu, de_log_std\n",
    "        \n",
    "\n",
    "    def decode2_3(self, z, y):\n",
    "        # concat latents and labels\n",
    "        h3 = F.relu(self.decoder_fc3(torch.cat([z, y], dim=1)))\n",
    "        # 这里decoder也是先decoder出来均值和方差，因为，后面计算loss函数要用\n",
    "        # 在decoder后再使用reparametrize重采样出来一个z放入下一层解码\n",
    "        de_mu = self.fc1_mu(h3)\n",
    "        de_log_std = self.fc1_log_std(h3)\n",
    "        return de_mu, de_log_std\n",
    "\n",
    "    def final_decode(self, z, y):\n",
    "        h3 = F.relu(self.decoder_fc3(torch.cat([z, y], dim=1)))\n",
    "        mu = self.decoder_mu(h3)\n",
    "        log_std = self.decoder_log_std(h3)\n",
    "        return mu, log_std\n",
    "\n",
    "    def reparametrize(self, mu, log_std):\n",
    "        std = torch.exp(log_std)\n",
    "        eps = torch.randn_like(std)  # simple from standard normal distribution\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # 第一次条件编码\n",
    "        mu_1, log_std_1 = self.encode1_2(self.encoder_fc1, x, y)\n",
    "        z_1 = self.reparametrize(mu_1, log_std_1)\n",
    "\n",
    "        # 第二次条件编码\n",
    "        mu_2, log_std_2 = self.encode1_2(self.encoder_fc2, z_1, y)\n",
    "        z2 = self.reparametrize(mu_2, log_std_2)\n",
    "\n",
    "        # 第三次条件编码\n",
    "        mu_3, log_std_3 = self.encode3(z2, y)\n",
    "        z3 = self.reparametrize(mu_3, log_std_3)\n",
    "\n",
    "        # 第一次条件解码\n",
    "        # 先解码出重构值，再根据重构值计算其均值和方差\n",
    "        de_mu3, de_log3 = self.decode1(z3, y)\n",
    "        recon3 = self.reparametrize(de_mu3, de_log3)\n",
    "\n",
    "        # 第二次条件解码\n",
    "        de_mu2, de_log2 = self.decode2_3(recon3, y)\n",
    "        recon2 = self.reparametrize(de_mu2, de_log2)\n",
    "\n",
    "        # 第三次条件解码\n",
    "        de_mu1, de_log1 = self.final_decode(recon2, y)\n",
    "        recon1 = self.reparametrize(de_mu1, de_log1)\n",
    "\n",
    "        # 根据计算loss函数所用到的内容\n",
    "        # 将编码和解码得到的均值打包成方差\n",
    "        en_mu = [mu_1, mu_2, mu_3]\n",
    "        de_mu = [de_mu3, de_mu2, de_mu1]\n",
    "        log_std = [log_std_1, log_std_2, log_std_3]\n",
    "        recon = [recon1, recon2, recon3]\n",
    "        z = [z_1, z2, z3]\n",
    "\n",
    "        #loss = self.loss_function(recon, x,  en_mu, de_mu, log_std)\n",
    "        return recon, z, en_mu, de_mu, log_std\n",
    "\n",
    "    def loss_function(self, recon, z, x, en_mu, de_mu, log_std) -> torch.Tensor:\n",
    "        # 计算重构误差\n",
    "        recon_loss = F.mse_loss(recon[0], x, reduction=\"sum\") \n",
    "        recon_loss = recon_loss + F.mse_loss(recon[1], z[0], reduction=\"sum\" )\n",
    "        recon_loss = recon_loss + F.mse_loss(recon[2], z[1], reduction=\"sum\" )\n",
    "        # 计算KL散度\n",
    "        kl_loss = torch.pow((en_mu[0] - de_mu[1]), 2)\n",
    "        kl_loss = kl_loss +  torch.pow((en_mu[1] - de_mu[0]), 2)\n",
    "        sum_log = torch.sum(log_std[0])+torch.sum(log_std[1])+torch.sum(log_std[2])\n",
    "        kl_loss = torch.sum(kl_loss) - sum_log\n",
    "        \n",
    "        # 计算整体的loss函数\n",
    "        loss =  kl_loss +recon_loss\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100], Batch[1/600], batch_loss:466697.531250\n",
      "Epoch[1/100], Batch[101/600], batch_loss:225582.156250\n",
      "Epoch[1/100], Batch[201/600], batch_loss:147525.703125\n",
      "Epoch[1/100], Batch[301/600], batch_loss:144416.484375\n",
      "Epoch[1/100], Batch[401/600], batch_loss:142659.656250\n",
      "Epoch[1/100], Batch[501/600], batch_loss:142290.015625\n",
      "======>epoch:1,\t epoch_average_batch_loss:180694.449401============ \n",
      "\n",
      "save: ./img/hcvae/hepoch1.png \n",
      "\n",
      "Epoch[2/100], Batch[1/600], batch_loss:141674.031250\n",
      "Epoch[2/100], Batch[101/600], batch_loss:141097.234375\n",
      "Epoch[2/100], Batch[201/600], batch_loss:140543.953125\n",
      "Epoch[2/100], Batch[301/600], batch_loss:141245.656250\n",
      "Epoch[2/100], Batch[401/600], batch_loss:140186.531250\n",
      "Epoch[2/100], Batch[501/600], batch_loss:140289.843750\n",
      "======>epoch:2,\t epoch_average_batch_loss:140679.224297============ \n",
      "\n",
      "Epoch[3/100], Batch[1/600], batch_loss:139896.468750\n",
      "Epoch[3/100], Batch[101/600], batch_loss:139006.375000\n",
      "Epoch[3/100], Batch[201/600], batch_loss:139587.609375\n",
      "Epoch[3/100], Batch[301/600], batch_loss:139229.156250\n",
      "Epoch[3/100], Batch[401/600], batch_loss:139376.281250\n",
      "Epoch[3/100], Batch[501/600], batch_loss:138652.546875\n",
      "======>epoch:3,\t epoch_average_batch_loss:139548.411484============ \n",
      "\n",
      "Epoch[4/100], Batch[1/600], batch_loss:138999.375000\n",
      "Epoch[4/100], Batch[101/600], batch_loss:138196.125000\n",
      "Epoch[4/100], Batch[201/600], batch_loss:138894.765625\n",
      "Epoch[4/100], Batch[301/600], batch_loss:138816.734375\n",
      "Epoch[4/100], Batch[401/600], batch_loss:138256.796875\n",
      "Epoch[4/100], Batch[501/600], batch_loss:137612.156250\n",
      "======>epoch:4,\t epoch_average_batch_loss:138546.897656============ \n",
      "\n",
      "Epoch[5/100], Batch[1/600], batch_loss:138208.562500\n",
      "Epoch[5/100], Batch[101/600], batch_loss:138007.156250\n",
      "Epoch[5/100], Batch[201/600], batch_loss:137290.312500\n",
      "Epoch[5/100], Batch[301/600], batch_loss:137942.875000\n",
      "Epoch[5/100], Batch[401/600], batch_loss:136780.703125\n",
      "Epoch[5/100], Batch[501/600], batch_loss:137405.984375\n",
      "======>epoch:5,\t epoch_average_batch_loss:137547.434974============ \n",
      "\n",
      "Epoch[6/100], Batch[1/600], batch_loss:136926.500000\n",
      "Epoch[6/100], Batch[101/600], batch_loss:137290.031250\n",
      "Epoch[6/100], Batch[201/600], batch_loss:136407.750000\n",
      "Epoch[6/100], Batch[301/600], batch_loss:137197.203125\n",
      "Epoch[6/100], Batch[401/600], batch_loss:136530.328125\n",
      "Epoch[6/100], Batch[501/600], batch_loss:136565.937500\n",
      "======>epoch:6,\t epoch_average_batch_loss:136757.381979============ \n",
      "\n",
      "Epoch[7/100], Batch[1/600], batch_loss:135849.968750\n",
      "Epoch[7/100], Batch[101/600], batch_loss:136709.171875\n",
      "Epoch[7/100], Batch[201/600], batch_loss:136842.156250\n",
      "Epoch[7/100], Batch[301/600], batch_loss:136024.562500\n",
      "Epoch[7/100], Batch[401/600], batch_loss:135792.031250\n",
      "Epoch[7/100], Batch[501/600], batch_loss:135891.343750\n",
      "======>epoch:7,\t epoch_average_batch_loss:136281.731771============ \n",
      "\n",
      "Epoch[8/100], Batch[1/600], batch_loss:135803.125000\n",
      "Epoch[8/100], Batch[101/600], batch_loss:136230.390625\n",
      "Epoch[8/100], Batch[201/600], batch_loss:136418.609375\n",
      "Epoch[8/100], Batch[301/600], batch_loss:136292.515625\n",
      "Epoch[8/100], Batch[401/600], batch_loss:135472.546875\n",
      "Epoch[8/100], Batch[501/600], batch_loss:136298.093750\n",
      "======>epoch:8,\t epoch_average_batch_loss:135953.637865============ \n",
      "\n",
      "Epoch[9/100], Batch[1/600], batch_loss:136331.109375\n",
      "Epoch[9/100], Batch[101/600], batch_loss:136349.031250\n",
      "Epoch[9/100], Batch[201/600], batch_loss:135340.625000\n",
      "Epoch[9/100], Batch[301/600], batch_loss:135572.000000\n",
      "Epoch[9/100], Batch[401/600], batch_loss:135598.046875\n",
      "Epoch[9/100], Batch[501/600], batch_loss:135487.406250\n",
      "======>epoch:9,\t epoch_average_batch_loss:135751.615521============ \n",
      "\n",
      "Epoch[10/100], Batch[1/600], batch_loss:135848.718750\n",
      "Epoch[10/100], Batch[101/600], batch_loss:135823.781250\n",
      "Epoch[10/100], Batch[201/600], batch_loss:135363.750000\n",
      "Epoch[10/100], Batch[301/600], batch_loss:135304.937500\n",
      "Epoch[10/100], Batch[401/600], batch_loss:135956.718750\n",
      "Epoch[10/100], Batch[501/600], batch_loss:135978.484375\n",
      "======>epoch:10,\t epoch_average_batch_loss:135620.712552============ \n",
      "\n",
      "Epoch[11/100], Batch[1/600], batch_loss:135894.046875\n",
      "Epoch[11/100], Batch[101/600], batch_loss:135604.953125\n",
      "Epoch[11/100], Batch[201/600], batch_loss:135471.531250\n",
      "Epoch[11/100], Batch[301/600], batch_loss:135735.250000\n",
      "Epoch[11/100], Batch[401/600], batch_loss:135663.921875\n",
      "Epoch[11/100], Batch[501/600], batch_loss:134875.250000\n",
      "======>epoch:11,\t epoch_average_batch_loss:135563.067266============ \n",
      "\n",
      "save: ./img/hcvae/hepoch11.png \n",
      "\n",
      "Epoch[12/100], Batch[1/600], batch_loss:135529.500000\n",
      "Epoch[12/100], Batch[101/600], batch_loss:135753.687500\n",
      "Epoch[12/100], Batch[201/600], batch_loss:135323.031250\n",
      "Epoch[12/100], Batch[301/600], batch_loss:135422.218750\n",
      "Epoch[12/100], Batch[401/600], batch_loss:136080.640625\n",
      "Epoch[12/100], Batch[501/600], batch_loss:135553.281250\n",
      "======>epoch:12,\t epoch_average_batch_loss:135447.991615============ \n",
      "\n",
      "Epoch[13/100], Batch[1/600], batch_loss:135364.968750\n",
      "Epoch[13/100], Batch[101/600], batch_loss:135144.781250\n",
      "Epoch[13/100], Batch[201/600], batch_loss:135479.734375\n",
      "Epoch[13/100], Batch[301/600], batch_loss:134880.734375\n",
      "Epoch[13/100], Batch[401/600], batch_loss:134898.640625\n",
      "Epoch[13/100], Batch[501/600], batch_loss:135624.531250\n",
      "======>epoch:13,\t epoch_average_batch_loss:135389.503672============ \n",
      "\n",
      "Epoch[14/100], Batch[1/600], batch_loss:135429.656250\n",
      "Epoch[14/100], Batch[101/600], batch_loss:135669.171875\n",
      "Epoch[14/100], Batch[201/600], batch_loss:135286.890625\n",
      "Epoch[14/100], Batch[301/600], batch_loss:135106.406250\n",
      "Epoch[14/100], Batch[401/600], batch_loss:135242.015625\n",
      "Epoch[14/100], Batch[501/600], batch_loss:135356.593750\n",
      "======>epoch:14,\t epoch_average_batch_loss:135297.451693============ \n",
      "\n",
      "Epoch[15/100], Batch[1/600], batch_loss:134927.812500\n",
      "Epoch[15/100], Batch[101/600], batch_loss:135792.828125\n",
      "Epoch[15/100], Batch[201/600], batch_loss:134558.281250\n",
      "Epoch[15/100], Batch[301/600], batch_loss:135629.734375\n",
      "Epoch[15/100], Batch[401/600], batch_loss:135205.093750\n",
      "Epoch[15/100], Batch[501/600], batch_loss:135310.093750\n",
      "======>epoch:15,\t epoch_average_batch_loss:135274.078750============ \n",
      "\n",
      "Epoch[16/100], Batch[1/600], batch_loss:134939.187500\n",
      "Epoch[16/100], Batch[101/600], batch_loss:135185.453125\n",
      "Epoch[16/100], Batch[201/600], batch_loss:134917.484375\n",
      "Epoch[16/100], Batch[301/600], batch_loss:134871.656250\n",
      "Epoch[16/100], Batch[401/600], batch_loss:134656.406250\n",
      "Epoch[16/100], Batch[501/600], batch_loss:134803.437500\n",
      "======>epoch:16,\t epoch_average_batch_loss:135193.516641============ \n",
      "\n",
      "Epoch[17/100], Batch[1/600], batch_loss:135047.828125\n",
      "Epoch[17/100], Batch[101/600], batch_loss:135260.375000\n",
      "Epoch[17/100], Batch[201/600], batch_loss:135491.812500\n",
      "Epoch[17/100], Batch[301/600], batch_loss:134374.843750\n",
      "Epoch[17/100], Batch[401/600], batch_loss:135361.718750\n",
      "Epoch[17/100], Batch[501/600], batch_loss:135590.671875\n",
      "======>epoch:17,\t epoch_average_batch_loss:135176.612031============ \n",
      "\n",
      "Epoch[18/100], Batch[1/600], batch_loss:134581.234375\n",
      "Epoch[18/100], Batch[101/600], batch_loss:134737.781250\n",
      "Epoch[18/100], Batch[201/600], batch_loss:135559.937500\n",
      "Epoch[18/100], Batch[301/600], batch_loss:135537.078125\n",
      "Epoch[18/100], Batch[401/600], batch_loss:135255.578125\n",
      "Epoch[18/100], Batch[501/600], batch_loss:134825.546875\n",
      "======>epoch:18,\t epoch_average_batch_loss:135098.568646============ \n",
      "\n",
      "Epoch[19/100], Batch[1/600], batch_loss:135097.656250\n",
      "Epoch[19/100], Batch[101/600], batch_loss:134842.687500\n",
      "Epoch[19/100], Batch[201/600], batch_loss:134643.671875\n",
      "Epoch[19/100], Batch[301/600], batch_loss:134549.000000\n",
      "Epoch[19/100], Batch[401/600], batch_loss:135190.531250\n",
      "Epoch[19/100], Batch[501/600], batch_loss:134925.328125\n",
      "======>epoch:19,\t epoch_average_batch_loss:135067.031875============ \n",
      "\n",
      "Epoch[20/100], Batch[1/600], batch_loss:134123.890625\n",
      "Epoch[20/100], Batch[101/600], batch_loss:135038.515625\n",
      "Epoch[20/100], Batch[201/600], batch_loss:134832.203125\n",
      "Epoch[20/100], Batch[301/600], batch_loss:134862.281250\n",
      "Epoch[20/100], Batch[401/600], batch_loss:134757.093750\n",
      "Epoch[20/100], Batch[501/600], batch_loss:135192.718750\n",
      "======>epoch:20,\t epoch_average_batch_loss:135070.643958============ \n",
      "\n",
      "Epoch[21/100], Batch[1/600], batch_loss:135638.812500\n",
      "Epoch[21/100], Batch[101/600], batch_loss:135373.000000\n",
      "Epoch[21/100], Batch[201/600], batch_loss:134708.562500\n",
      "Epoch[21/100], Batch[301/600], batch_loss:134759.656250\n",
      "Epoch[21/100], Batch[401/600], batch_loss:135300.750000\n",
      "Epoch[21/100], Batch[501/600], batch_loss:135200.531250\n",
      "======>epoch:21,\t epoch_average_batch_loss:135026.453854============ \n",
      "\n",
      "save: ./img/hcvae/hepoch21.png \n",
      "\n",
      "Epoch[22/100], Batch[1/600], batch_loss:134997.812500\n",
      "Epoch[22/100], Batch[101/600], batch_loss:135010.046875\n",
      "Epoch[22/100], Batch[201/600], batch_loss:134760.093750\n",
      "Epoch[22/100], Batch[301/600], batch_loss:136564.937500\n",
      "Epoch[22/100], Batch[401/600], batch_loss:134286.703125\n",
      "Epoch[22/100], Batch[501/600], batch_loss:135309.531250\n",
      "======>epoch:22,\t epoch_average_batch_loss:134981.765156============ \n",
      "\n",
      "Epoch[23/100], Batch[1/600], batch_loss:134881.046875\n",
      "Epoch[23/100], Batch[101/600], batch_loss:135038.609375\n",
      "Epoch[23/100], Batch[201/600], batch_loss:134569.187500\n",
      "Epoch[23/100], Batch[301/600], batch_loss:134854.312500\n",
      "Epoch[23/100], Batch[401/600], batch_loss:134951.656250\n",
      "Epoch[23/100], Batch[501/600], batch_loss:134851.218750\n",
      "======>epoch:23,\t epoch_average_batch_loss:134957.856719============ \n",
      "\n",
      "Epoch[24/100], Batch[1/600], batch_loss:135141.406250\n",
      "Epoch[24/100], Batch[101/600], batch_loss:134883.421875\n",
      "Epoch[24/100], Batch[201/600], batch_loss:135012.734375\n",
      "Epoch[24/100], Batch[301/600], batch_loss:134895.000000\n",
      "Epoch[24/100], Batch[401/600], batch_loss:134789.140625\n",
      "Epoch[24/100], Batch[501/600], batch_loss:135106.281250\n",
      "======>epoch:24,\t epoch_average_batch_loss:134932.094609============ \n",
      "\n",
      "Epoch[25/100], Batch[1/600], batch_loss:135383.328125\n",
      "Epoch[25/100], Batch[101/600], batch_loss:135006.125000\n",
      "Epoch[25/100], Batch[201/600], batch_loss:134298.796875\n",
      "Epoch[25/100], Batch[301/600], batch_loss:135138.406250\n",
      "Epoch[25/100], Batch[401/600], batch_loss:134430.468750\n",
      "Epoch[25/100], Batch[501/600], batch_loss:135265.937500\n",
      "======>epoch:25,\t epoch_average_batch_loss:134948.809688============ \n",
      "\n",
      "Epoch[26/100], Batch[1/600], batch_loss:134495.156250\n",
      "Epoch[26/100], Batch[101/600], batch_loss:134979.015625\n",
      "Epoch[26/100], Batch[201/600], batch_loss:134731.203125\n",
      "Epoch[26/100], Batch[301/600], batch_loss:134382.562500\n",
      "Epoch[26/100], Batch[401/600], batch_loss:134894.109375\n",
      "Epoch[26/100], Batch[501/600], batch_loss:134849.640625\n",
      "======>epoch:26,\t epoch_average_batch_loss:134914.241901============ \n",
      "\n",
      "Epoch[27/100], Batch[1/600], batch_loss:135313.187500\n",
      "Epoch[27/100], Batch[101/600], batch_loss:134917.187500\n",
      "Epoch[27/100], Batch[201/600], batch_loss:134917.000000\n",
      "Epoch[27/100], Batch[301/600], batch_loss:134684.937500\n",
      "Epoch[27/100], Batch[401/600], batch_loss:135404.437500\n",
      "Epoch[27/100], Batch[501/600], batch_loss:134445.375000\n",
      "======>epoch:27,\t epoch_average_batch_loss:134881.868203============ \n",
      "\n",
      "Epoch[28/100], Batch[1/600], batch_loss:134482.328125\n",
      "Epoch[28/100], Batch[101/600], batch_loss:134917.343750\n",
      "Epoch[28/100], Batch[201/600], batch_loss:135061.812500\n",
      "Epoch[28/100], Batch[301/600], batch_loss:135046.531250\n",
      "Epoch[28/100], Batch[401/600], batch_loss:134784.734375\n",
      "Epoch[28/100], Batch[501/600], batch_loss:135056.468750\n",
      "======>epoch:28,\t epoch_average_batch_loss:134847.486563============ \n",
      "\n",
      "Epoch[29/100], Batch[1/600], batch_loss:134717.796875\n",
      "Epoch[29/100], Batch[101/600], batch_loss:134549.343750\n",
      "Epoch[29/100], Batch[201/600], batch_loss:135480.093750\n",
      "Epoch[29/100], Batch[301/600], batch_loss:134584.546875\n",
      "Epoch[29/100], Batch[401/600], batch_loss:134418.281250\n",
      "Epoch[29/100], Batch[501/600], batch_loss:134839.187500\n",
      "======>epoch:29,\t epoch_average_batch_loss:134847.296198============ \n",
      "\n",
      "Epoch[30/100], Batch[1/600], batch_loss:134552.578125\n",
      "Epoch[30/100], Batch[101/600], batch_loss:134184.156250\n",
      "Epoch[30/100], Batch[201/600], batch_loss:134818.359375\n",
      "Epoch[30/100], Batch[301/600], batch_loss:134917.437500\n",
      "Epoch[30/100], Batch[401/600], batch_loss:134319.437500\n",
      "Epoch[30/100], Batch[501/600], batch_loss:134767.046875\n",
      "======>epoch:30,\t epoch_average_batch_loss:134838.037500============ \n",
      "\n",
      "Epoch[31/100], Batch[1/600], batch_loss:134278.703125\n",
      "Epoch[31/100], Batch[101/600], batch_loss:135093.734375\n",
      "Epoch[31/100], Batch[201/600], batch_loss:134842.312500\n",
      "Epoch[31/100], Batch[301/600], batch_loss:134840.625000\n",
      "Epoch[31/100], Batch[401/600], batch_loss:134733.890625\n",
      "Epoch[31/100], Batch[501/600], batch_loss:134438.687500\n",
      "======>epoch:31,\t epoch_average_batch_loss:134874.054219============ \n",
      "\n",
      "save: ./img/hcvae/hepoch31.png \n",
      "\n",
      "Epoch[32/100], Batch[1/600], batch_loss:135361.093750\n",
      "Epoch[32/100], Batch[101/600], batch_loss:134900.453125\n",
      "Epoch[32/100], Batch[201/600], batch_loss:134764.343750\n",
      "Epoch[32/100], Batch[301/600], batch_loss:135336.562500\n",
      "Epoch[32/100], Batch[401/600], batch_loss:134531.843750\n",
      "Epoch[32/100], Batch[501/600], batch_loss:134751.046875\n",
      "======>epoch:32,\t epoch_average_batch_loss:134794.115391============ \n",
      "\n",
      "Epoch[33/100], Batch[1/600], batch_loss:134808.156250\n",
      "Epoch[33/100], Batch[101/600], batch_loss:134716.031250\n",
      "Epoch[33/100], Batch[201/600], batch_loss:135805.343750\n",
      "Epoch[33/100], Batch[301/600], batch_loss:134232.390625\n",
      "Epoch[33/100], Batch[401/600], batch_loss:134791.125000\n",
      "Epoch[33/100], Batch[501/600], batch_loss:135273.343750\n",
      "======>epoch:33,\t epoch_average_batch_loss:134800.399688============ \n",
      "\n",
      "Epoch[34/100], Batch[1/600], batch_loss:134658.984375\n",
      "Epoch[34/100], Batch[101/600], batch_loss:135037.187500\n",
      "Epoch[34/100], Batch[201/600], batch_loss:135010.140625\n",
      "Epoch[34/100], Batch[301/600], batch_loss:134816.187500\n",
      "Epoch[34/100], Batch[401/600], batch_loss:134895.375000\n",
      "Epoch[34/100], Batch[501/600], batch_loss:135064.000000\n",
      "======>epoch:34,\t epoch_average_batch_loss:134790.845469============ \n",
      "\n",
      "Epoch[35/100], Batch[1/600], batch_loss:134608.812500\n",
      "Epoch[35/100], Batch[101/600], batch_loss:134879.296875\n",
      "Epoch[35/100], Batch[201/600], batch_loss:134978.234375\n",
      "Epoch[35/100], Batch[301/600], batch_loss:135104.421875\n",
      "Epoch[35/100], Batch[401/600], batch_loss:135102.281250\n",
      "Epoch[35/100], Batch[501/600], batch_loss:134762.609375\n",
      "======>epoch:35,\t epoch_average_batch_loss:134823.189714============ \n",
      "\n",
      "Epoch[36/100], Batch[1/600], batch_loss:134944.328125\n",
      "Epoch[36/100], Batch[101/600], batch_loss:134243.031250\n",
      "Epoch[36/100], Batch[201/600], batch_loss:135007.500000\n",
      "Epoch[36/100], Batch[301/600], batch_loss:135413.250000\n",
      "Epoch[36/100], Batch[401/600], batch_loss:135112.312500\n",
      "Epoch[36/100], Batch[501/600], batch_loss:135091.390625\n",
      "======>epoch:36,\t epoch_average_batch_loss:134749.784245============ \n",
      "\n",
      "Epoch[37/100], Batch[1/600], batch_loss:135477.203125\n",
      "Epoch[37/100], Batch[101/600], batch_loss:134683.718750\n",
      "Epoch[37/100], Batch[201/600], batch_loss:135119.500000\n",
      "Epoch[37/100], Batch[301/600], batch_loss:134201.937500\n",
      "Epoch[37/100], Batch[401/600], batch_loss:135257.109375\n",
      "Epoch[37/100], Batch[501/600], batch_loss:134544.828125\n",
      "======>epoch:37,\t epoch_average_batch_loss:134778.662891============ \n",
      "\n",
      "Epoch[38/100], Batch[1/600], batch_loss:135194.968750\n",
      "Epoch[38/100], Batch[101/600], batch_loss:134794.906250\n",
      "Epoch[38/100], Batch[201/600], batch_loss:134457.968750\n",
      "Epoch[38/100], Batch[301/600], batch_loss:134566.078125\n",
      "Epoch[38/100], Batch[401/600], batch_loss:134621.796875\n",
      "Epoch[38/100], Batch[501/600], batch_loss:134378.234375\n",
      "======>epoch:38,\t epoch_average_batch_loss:134702.021510============ \n",
      "\n",
      "Epoch[39/100], Batch[1/600], batch_loss:134772.515625\n",
      "Epoch[39/100], Batch[101/600], batch_loss:135244.156250\n",
      "Epoch[39/100], Batch[201/600], batch_loss:135087.062500\n",
      "Epoch[39/100], Batch[301/600], batch_loss:134176.406250\n",
      "Epoch[39/100], Batch[401/600], batch_loss:134655.687500\n",
      "Epoch[39/100], Batch[501/600], batch_loss:134878.109375\n",
      "======>epoch:39,\t epoch_average_batch_loss:134729.735130============ \n",
      "\n",
      "Epoch[40/100], Batch[1/600], batch_loss:134996.125000\n",
      "Epoch[40/100], Batch[101/600], batch_loss:135256.531250\n",
      "Epoch[40/100], Batch[201/600], batch_loss:134262.937500\n",
      "Epoch[40/100], Batch[301/600], batch_loss:134790.437500\n",
      "Epoch[40/100], Batch[401/600], batch_loss:134931.890625\n",
      "Epoch[40/100], Batch[501/600], batch_loss:134941.265625\n",
      "======>epoch:40,\t epoch_average_batch_loss:134690.768906============ \n",
      "\n",
      "Epoch[41/100], Batch[1/600], batch_loss:134661.125000\n",
      "Epoch[41/100], Batch[101/600], batch_loss:134613.656250\n",
      "Epoch[41/100], Batch[201/600], batch_loss:134330.484375\n",
      "Epoch[41/100], Batch[301/600], batch_loss:134650.468750\n",
      "Epoch[41/100], Batch[401/600], batch_loss:134245.203125\n",
      "Epoch[41/100], Batch[501/600], batch_loss:134690.609375\n",
      "======>epoch:41,\t epoch_average_batch_loss:134697.755208============ \n",
      "\n",
      "save: ./img/hcvae/hepoch41.png \n",
      "\n",
      "Epoch[42/100], Batch[1/600], batch_loss:134693.625000\n",
      "Epoch[42/100], Batch[101/600], batch_loss:134819.843750\n",
      "Epoch[42/100], Batch[201/600], batch_loss:134522.781250\n",
      "Epoch[42/100], Batch[301/600], batch_loss:134526.140625\n",
      "Epoch[42/100], Batch[401/600], batch_loss:134523.343750\n",
      "Epoch[42/100], Batch[501/600], batch_loss:134358.250000\n",
      "======>epoch:42,\t epoch_average_batch_loss:134656.954167============ \n",
      "\n",
      "Epoch[43/100], Batch[1/600], batch_loss:134407.562500\n",
      "Epoch[43/100], Batch[101/600], batch_loss:134931.312500\n",
      "Epoch[43/100], Batch[201/600], batch_loss:134601.968750\n",
      "Epoch[43/100], Batch[301/600], batch_loss:135363.531250\n",
      "Epoch[43/100], Batch[401/600], batch_loss:134402.750000\n",
      "Epoch[43/100], Batch[501/600], batch_loss:135262.125000\n",
      "======>epoch:43,\t epoch_average_batch_loss:134724.975078============ \n",
      "\n",
      "Epoch[44/100], Batch[1/600], batch_loss:134358.406250\n",
      "Epoch[44/100], Batch[101/600], batch_loss:134551.484375\n",
      "Epoch[44/100], Batch[201/600], batch_loss:134487.343750\n",
      "Epoch[44/100], Batch[301/600], batch_loss:135038.562500\n",
      "Epoch[44/100], Batch[401/600], batch_loss:135252.468750\n",
      "Epoch[44/100], Batch[501/600], batch_loss:134995.484375\n",
      "======>epoch:44,\t epoch_average_batch_loss:134669.745781============ \n",
      "\n",
      "Epoch[45/100], Batch[1/600], batch_loss:134216.000000\n",
      "Epoch[45/100], Batch[101/600], batch_loss:134246.468750\n",
      "Epoch[45/100], Batch[201/600], batch_loss:134753.843750\n",
      "Epoch[45/100], Batch[301/600], batch_loss:134625.718750\n",
      "Epoch[45/100], Batch[401/600], batch_loss:134452.875000\n",
      "Epoch[45/100], Batch[501/600], batch_loss:134305.343750\n",
      "======>epoch:45,\t epoch_average_batch_loss:134668.935547============ \n",
      "\n",
      "Epoch[46/100], Batch[1/600], batch_loss:135162.390625\n",
      "Epoch[46/100], Batch[101/600], batch_loss:135276.093750\n",
      "Epoch[46/100], Batch[201/600], batch_loss:135188.875000\n",
      "Epoch[46/100], Batch[301/600], batch_loss:134813.312500\n",
      "Epoch[46/100], Batch[401/600], batch_loss:134979.343750\n",
      "Epoch[46/100], Batch[501/600], batch_loss:135282.515625\n",
      "======>epoch:46,\t epoch_average_batch_loss:134712.953411============ \n",
      "\n",
      "Epoch[47/100], Batch[1/600], batch_loss:134781.312500\n",
      "Epoch[47/100], Batch[101/600], batch_loss:134716.312500\n",
      "Epoch[47/100], Batch[201/600], batch_loss:134815.250000\n",
      "Epoch[47/100], Batch[301/600], batch_loss:134277.062500\n",
      "Epoch[47/100], Batch[401/600], batch_loss:134808.109375\n",
      "Epoch[47/100], Batch[501/600], batch_loss:134869.328125\n",
      "======>epoch:47,\t epoch_average_batch_loss:134660.390495============ \n",
      "\n",
      "Epoch[48/100], Batch[1/600], batch_loss:134964.328125\n",
      "Epoch[48/100], Batch[101/600], batch_loss:134839.578125\n",
      "Epoch[48/100], Batch[201/600], batch_loss:135078.562500\n",
      "Epoch[48/100], Batch[301/600], batch_loss:134609.671875\n",
      "Epoch[48/100], Batch[401/600], batch_loss:134943.718750\n",
      "Epoch[48/100], Batch[501/600], batch_loss:133989.062500\n",
      "======>epoch:48,\t epoch_average_batch_loss:134635.032760============ \n",
      "\n",
      "Epoch[49/100], Batch[1/600], batch_loss:134446.531250\n",
      "Epoch[49/100], Batch[101/600], batch_loss:135179.796875\n",
      "Epoch[49/100], Batch[201/600], batch_loss:134958.296875\n",
      "Epoch[49/100], Batch[301/600], batch_loss:134182.687500\n",
      "Epoch[49/100], Batch[401/600], batch_loss:134761.453125\n",
      "Epoch[49/100], Batch[501/600], batch_loss:134547.578125\n",
      "======>epoch:49,\t epoch_average_batch_loss:134612.827995============ \n",
      "\n",
      "Epoch[50/100], Batch[1/600], batch_loss:134540.781250\n",
      "Epoch[50/100], Batch[101/600], batch_loss:134789.031250\n",
      "Epoch[50/100], Batch[201/600], batch_loss:134204.515625\n",
      "Epoch[50/100], Batch[301/600], batch_loss:134229.562500\n",
      "Epoch[50/100], Batch[401/600], batch_loss:134154.984375\n",
      "Epoch[50/100], Batch[501/600], batch_loss:134452.625000\n",
      "======>epoch:50,\t epoch_average_batch_loss:134614.647604============ \n",
      "\n",
      "Epoch[51/100], Batch[1/600], batch_loss:134574.875000\n",
      "Epoch[51/100], Batch[101/600], batch_loss:134363.437500\n",
      "Epoch[51/100], Batch[201/600], batch_loss:134222.750000\n",
      "Epoch[51/100], Batch[301/600], batch_loss:134665.812500\n",
      "Epoch[51/100], Batch[401/600], batch_loss:135221.781250\n",
      "Epoch[51/100], Batch[501/600], batch_loss:134727.812500\n",
      "======>epoch:51,\t epoch_average_batch_loss:134576.718490============ \n",
      "\n",
      "save: ./img/hcvae/hepoch51.png \n",
      "\n",
      "Epoch[52/100], Batch[1/600], batch_loss:134937.937500\n",
      "Epoch[52/100], Batch[101/600], batch_loss:134453.328125\n",
      "Epoch[52/100], Batch[201/600], batch_loss:135190.125000\n",
      "Epoch[52/100], Batch[301/600], batch_loss:134932.625000\n",
      "Epoch[52/100], Batch[401/600], batch_loss:135266.468750\n",
      "Epoch[52/100], Batch[501/600], batch_loss:134129.625000\n",
      "======>epoch:52,\t epoch_average_batch_loss:134566.865625============ \n",
      "\n",
      "Epoch[53/100], Batch[1/600], batch_loss:134600.703125\n",
      "Epoch[53/100], Batch[101/600], batch_loss:133986.359375\n",
      "Epoch[53/100], Batch[201/600], batch_loss:134344.562500\n",
      "Epoch[53/100], Batch[301/600], batch_loss:134419.500000\n",
      "Epoch[53/100], Batch[401/600], batch_loss:134001.578125\n",
      "Epoch[53/100], Batch[501/600], batch_loss:134209.937500\n",
      "======>epoch:53,\t epoch_average_batch_loss:134521.189505============ \n",
      "\n",
      "Epoch[54/100], Batch[1/600], batch_loss:134096.671875\n",
      "Epoch[54/100], Batch[101/600], batch_loss:135083.406250\n",
      "Epoch[54/100], Batch[201/600], batch_loss:135069.125000\n",
      "Epoch[54/100], Batch[301/600], batch_loss:134736.625000\n",
      "Epoch[54/100], Batch[401/600], batch_loss:135006.796875\n",
      "Epoch[54/100], Batch[501/600], batch_loss:134266.828125\n",
      "======>epoch:54,\t epoch_average_batch_loss:134631.276745============ \n",
      "\n",
      "Epoch[55/100], Batch[1/600], batch_loss:135202.046875\n",
      "Epoch[55/100], Batch[101/600], batch_loss:134956.093750\n",
      "Epoch[55/100], Batch[201/600], batch_loss:134281.250000\n",
      "Epoch[55/100], Batch[301/600], batch_loss:134567.812500\n",
      "Epoch[55/100], Batch[401/600], batch_loss:134739.953125\n",
      "Epoch[55/100], Batch[501/600], batch_loss:134546.390625\n",
      "======>epoch:55,\t epoch_average_batch_loss:134576.071615============ \n",
      "\n",
      "Epoch[56/100], Batch[1/600], batch_loss:134418.750000\n",
      "Epoch[56/100], Batch[101/600], batch_loss:134193.687500\n",
      "Epoch[56/100], Batch[201/600], batch_loss:134098.609375\n",
      "Epoch[56/100], Batch[301/600], batch_loss:134743.968750\n",
      "Epoch[56/100], Batch[401/600], batch_loss:134843.093750\n",
      "Epoch[56/100], Batch[501/600], batch_loss:134846.937500\n",
      "======>epoch:56,\t epoch_average_batch_loss:134567.274844============ \n",
      "\n",
      "Epoch[57/100], Batch[1/600], batch_loss:134697.375000\n",
      "Epoch[57/100], Batch[101/600], batch_loss:134459.562500\n",
      "Epoch[57/100], Batch[201/600], batch_loss:134735.171875\n",
      "Epoch[57/100], Batch[301/600], batch_loss:134212.390625\n",
      "Epoch[57/100], Batch[401/600], batch_loss:134857.375000\n",
      "Epoch[57/100], Batch[501/600], batch_loss:134109.843750\n",
      "======>epoch:57,\t epoch_average_batch_loss:134516.040417============ \n",
      "\n",
      "Epoch[58/100], Batch[1/600], batch_loss:135123.468750\n",
      "Epoch[58/100], Batch[101/600], batch_loss:134521.328125\n",
      "Epoch[58/100], Batch[201/600], batch_loss:134671.031250\n",
      "Epoch[58/100], Batch[301/600], batch_loss:134258.531250\n",
      "Epoch[58/100], Batch[401/600], batch_loss:134044.218750\n",
      "Epoch[58/100], Batch[501/600], batch_loss:134759.093750\n",
      "======>epoch:58,\t epoch_average_batch_loss:134521.984245============ \n",
      "\n",
      "Epoch[59/100], Batch[1/600], batch_loss:134978.250000\n",
      "Epoch[59/100], Batch[101/600], batch_loss:133956.390625\n",
      "Epoch[59/100], Batch[201/600], batch_loss:134409.375000\n",
      "Epoch[59/100], Batch[301/600], batch_loss:134458.062500\n",
      "Epoch[59/100], Batch[401/600], batch_loss:134897.781250\n",
      "Epoch[59/100], Batch[501/600], batch_loss:134849.421875\n",
      "======>epoch:59,\t epoch_average_batch_loss:134489.807630============ \n",
      "\n",
      "Epoch[60/100], Batch[1/600], batch_loss:135145.046875\n",
      "Epoch[60/100], Batch[101/600], batch_loss:134694.187500\n",
      "Epoch[60/100], Batch[201/600], batch_loss:134355.234375\n",
      "Epoch[60/100], Batch[301/600], batch_loss:134121.500000\n",
      "Epoch[60/100], Batch[401/600], batch_loss:134271.703125\n",
      "Epoch[60/100], Batch[501/600], batch_loss:134518.578125\n",
      "======>epoch:60,\t epoch_average_batch_loss:134540.275078============ \n",
      "\n",
      "Epoch[61/100], Batch[1/600], batch_loss:134598.843750\n",
      "Epoch[61/100], Batch[101/600], batch_loss:134367.312500\n",
      "Epoch[61/100], Batch[201/600], batch_loss:133864.156250\n",
      "Epoch[61/100], Batch[301/600], batch_loss:134599.531250\n",
      "Epoch[61/100], Batch[401/600], batch_loss:134323.250000\n",
      "Epoch[61/100], Batch[501/600], batch_loss:134696.093750\n",
      "======>epoch:61,\t epoch_average_batch_loss:134532.114870============ \n",
      "\n",
      "save: ./img/hcvae/hepoch61.png \n",
      "\n",
      "Epoch[62/100], Batch[1/600], batch_loss:134890.109375\n",
      "Epoch[62/100], Batch[101/600], batch_loss:134583.343750\n",
      "Epoch[62/100], Batch[201/600], batch_loss:134048.406250\n",
      "Epoch[62/100], Batch[301/600], batch_loss:134593.000000\n",
      "Epoch[62/100], Batch[401/600], batch_loss:134306.000000\n",
      "Epoch[62/100], Batch[501/600], batch_loss:134115.593750\n",
      "======>epoch:62,\t epoch_average_batch_loss:134471.007656============ \n",
      "\n",
      "Epoch[63/100], Batch[1/600], batch_loss:134629.312500\n",
      "Epoch[63/100], Batch[101/600], batch_loss:134479.453125\n",
      "Epoch[63/100], Batch[201/600], batch_loss:134267.671875\n",
      "Epoch[63/100], Batch[301/600], batch_loss:134763.843750\n",
      "Epoch[63/100], Batch[401/600], batch_loss:134747.921875\n",
      "Epoch[63/100], Batch[501/600], batch_loss:134804.750000\n",
      "======>epoch:63,\t epoch_average_batch_loss:134457.768411============ \n",
      "\n",
      "Epoch[64/100], Batch[1/600], batch_loss:134561.765625\n",
      "Epoch[64/100], Batch[101/600], batch_loss:134155.312500\n",
      "Epoch[64/100], Batch[201/600], batch_loss:134861.453125\n",
      "Epoch[64/100], Batch[301/600], batch_loss:134126.656250\n",
      "Epoch[64/100], Batch[401/600], batch_loss:134145.093750\n",
      "Epoch[64/100], Batch[501/600], batch_loss:134506.562500\n",
      "======>epoch:64,\t epoch_average_batch_loss:134429.201953============ \n",
      "\n",
      "Epoch[65/100], Batch[1/600], batch_loss:134534.656250\n",
      "Epoch[65/100], Batch[101/600], batch_loss:134397.984375\n",
      "Epoch[65/100], Batch[201/600], batch_loss:134341.718750\n",
      "Epoch[65/100], Batch[301/600], batch_loss:134902.343750\n",
      "Epoch[65/100], Batch[401/600], batch_loss:134777.562500\n",
      "Epoch[65/100], Batch[501/600], batch_loss:134320.906250\n",
      "======>epoch:65,\t epoch_average_batch_loss:134427.289557============ \n",
      "\n",
      "Epoch[66/100], Batch[1/600], batch_loss:134104.125000\n",
      "Epoch[66/100], Batch[101/600], batch_loss:134709.765625\n",
      "Epoch[66/100], Batch[201/600], batch_loss:134962.437500\n",
      "Epoch[66/100], Batch[301/600], batch_loss:134337.687500\n",
      "Epoch[66/100], Batch[401/600], batch_loss:134318.953125\n",
      "Epoch[66/100], Batch[501/600], batch_loss:134307.546875\n",
      "======>epoch:66,\t epoch_average_batch_loss:134408.861849============ \n",
      "\n",
      "Epoch[67/100], Batch[1/600], batch_loss:134158.890625\n",
      "Epoch[67/100], Batch[101/600], batch_loss:135168.781250\n",
      "Epoch[67/100], Batch[201/600], batch_loss:133959.765625\n",
      "Epoch[67/100], Batch[301/600], batch_loss:134249.093750\n",
      "Epoch[67/100], Batch[401/600], batch_loss:134671.140625\n",
      "Epoch[67/100], Batch[501/600], batch_loss:134343.500000\n",
      "======>epoch:67,\t epoch_average_batch_loss:134426.592422============ \n",
      "\n",
      "Epoch[68/100], Batch[1/600], batch_loss:134151.125000\n",
      "Epoch[68/100], Batch[101/600], batch_loss:134022.921875\n",
      "Epoch[68/100], Batch[201/600], batch_loss:134200.343750\n",
      "Epoch[68/100], Batch[301/600], batch_loss:134684.187500\n",
      "Epoch[68/100], Batch[401/600], batch_loss:135045.203125\n",
      "Epoch[68/100], Batch[501/600], batch_loss:134304.296875\n",
      "======>epoch:68,\t epoch_average_batch_loss:134405.475365============ \n",
      "\n",
      "Epoch[69/100], Batch[1/600], batch_loss:134400.484375\n",
      "Epoch[69/100], Batch[101/600], batch_loss:134656.984375\n",
      "Epoch[69/100], Batch[201/600], batch_loss:134378.218750\n",
      "Epoch[69/100], Batch[301/600], batch_loss:134827.031250\n",
      "Epoch[69/100], Batch[401/600], batch_loss:134686.296875\n",
      "Epoch[69/100], Batch[501/600], batch_loss:134517.015625\n",
      "======>epoch:69,\t epoch_average_batch_loss:134418.191562============ \n",
      "\n",
      "Epoch[70/100], Batch[1/600], batch_loss:134622.812500\n",
      "Epoch[70/100], Batch[101/600], batch_loss:134454.312500\n",
      "Epoch[70/100], Batch[201/600], batch_loss:134356.843750\n",
      "Epoch[70/100], Batch[301/600], batch_loss:134076.531250\n",
      "Epoch[70/100], Batch[401/600], batch_loss:134127.000000\n",
      "Epoch[70/100], Batch[501/600], batch_loss:134768.046875\n",
      "======>epoch:70,\t epoch_average_batch_loss:134364.874583============ \n",
      "\n",
      "Epoch[71/100], Batch[1/600], batch_loss:134945.984375\n",
      "Epoch[71/100], Batch[101/600], batch_loss:134397.828125\n",
      "Epoch[71/100], Batch[201/600], batch_loss:134274.343750\n",
      "Epoch[71/100], Batch[301/600], batch_loss:134176.765625\n",
      "Epoch[71/100], Batch[401/600], batch_loss:134599.531250\n",
      "Epoch[71/100], Batch[501/600], batch_loss:136609.671875\n",
      "======>epoch:71,\t epoch_average_batch_loss:134375.122266============ \n",
      "\n",
      "save: ./img/hcvae/hepoch71.png \n",
      "\n",
      "Epoch[72/100], Batch[1/600], batch_loss:134465.531250\n",
      "Epoch[72/100], Batch[101/600], batch_loss:134383.828125\n",
      "Epoch[72/100], Batch[201/600], batch_loss:134190.406250\n",
      "Epoch[72/100], Batch[301/600], batch_loss:134624.875000\n",
      "Epoch[72/100], Batch[401/600], batch_loss:134029.984375\n",
      "Epoch[72/100], Batch[501/600], batch_loss:134890.281250\n",
      "======>epoch:72,\t epoch_average_batch_loss:134356.725781============ \n",
      "\n",
      "Epoch[73/100], Batch[1/600], batch_loss:134477.328125\n",
      "Epoch[73/100], Batch[101/600], batch_loss:133947.703125\n",
      "Epoch[73/100], Batch[201/600], batch_loss:134071.843750\n",
      "Epoch[73/100], Batch[301/600], batch_loss:134367.171875\n",
      "Epoch[73/100], Batch[401/600], batch_loss:134184.828125\n",
      "Epoch[73/100], Batch[501/600], batch_loss:134365.218750\n",
      "======>epoch:73,\t epoch_average_batch_loss:134357.444505============ \n",
      "\n",
      "Epoch[74/100], Batch[1/600], batch_loss:133887.328125\n",
      "Epoch[74/100], Batch[101/600], batch_loss:134549.515625\n",
      "Epoch[74/100], Batch[201/600], batch_loss:134355.625000\n",
      "Epoch[74/100], Batch[301/600], batch_loss:134157.937500\n",
      "Epoch[74/100], Batch[401/600], batch_loss:134186.328125\n",
      "Epoch[74/100], Batch[501/600], batch_loss:134871.765625\n",
      "======>epoch:74,\t epoch_average_batch_loss:134304.870938============ \n",
      "\n",
      "Epoch[75/100], Batch[1/600], batch_loss:134214.359375\n",
      "Epoch[75/100], Batch[101/600], batch_loss:134487.640625\n",
      "Epoch[75/100], Batch[201/600], batch_loss:134088.468750\n",
      "Epoch[75/100], Batch[301/600], batch_loss:134301.140625\n",
      "Epoch[75/100], Batch[401/600], batch_loss:134524.890625\n",
      "Epoch[75/100], Batch[501/600], batch_loss:134261.046875\n",
      "======>epoch:75,\t epoch_average_batch_loss:134338.913880============ \n",
      "\n",
      "Epoch[76/100], Batch[1/600], batch_loss:134483.671875\n",
      "Epoch[76/100], Batch[101/600], batch_loss:134120.765625\n",
      "Epoch[76/100], Batch[201/600], batch_loss:134556.453125\n",
      "Epoch[76/100], Batch[301/600], batch_loss:134211.890625\n",
      "Epoch[76/100], Batch[401/600], batch_loss:135004.234375\n",
      "Epoch[76/100], Batch[501/600], batch_loss:134459.312500\n",
      "======>epoch:76,\t epoch_average_batch_loss:134347.722135============ \n",
      "\n",
      "Epoch[77/100], Batch[1/600], batch_loss:134463.843750\n",
      "Epoch[77/100], Batch[101/600], batch_loss:133768.265625\n",
      "Epoch[77/100], Batch[201/600], batch_loss:134606.421875\n",
      "Epoch[77/100], Batch[301/600], batch_loss:134683.593750\n",
      "Epoch[77/100], Batch[401/600], batch_loss:133898.906250\n",
      "Epoch[77/100], Batch[501/600], batch_loss:134577.000000\n",
      "======>epoch:77,\t epoch_average_batch_loss:134324.538333============ \n",
      "\n",
      "Epoch[78/100], Batch[1/600], batch_loss:134620.046875\n",
      "Epoch[78/100], Batch[101/600], batch_loss:134134.578125\n",
      "Epoch[78/100], Batch[201/600], batch_loss:134206.312500\n",
      "Epoch[78/100], Batch[301/600], batch_loss:133853.312500\n",
      "Epoch[78/100], Batch[401/600], batch_loss:134467.265625\n",
      "Epoch[78/100], Batch[501/600], batch_loss:134160.640625\n",
      "======>epoch:78,\t epoch_average_batch_loss:134315.168047============ \n",
      "\n",
      "Epoch[79/100], Batch[1/600], batch_loss:134884.453125\n",
      "Epoch[79/100], Batch[101/600], batch_loss:134481.187500\n",
      "Epoch[79/100], Batch[201/600], batch_loss:134309.890625\n",
      "Epoch[79/100], Batch[301/600], batch_loss:134181.937500\n",
      "Epoch[79/100], Batch[401/600], batch_loss:134737.578125\n",
      "Epoch[79/100], Batch[501/600], batch_loss:134120.250000\n",
      "======>epoch:79,\t epoch_average_batch_loss:134312.064818============ \n",
      "\n",
      "Epoch[80/100], Batch[1/600], batch_loss:134383.171875\n",
      "Epoch[80/100], Batch[101/600], batch_loss:133955.500000\n",
      "Epoch[80/100], Batch[201/600], batch_loss:133241.578125\n",
      "Epoch[80/100], Batch[301/600], batch_loss:134751.140625\n",
      "Epoch[80/100], Batch[401/600], batch_loss:134694.187500\n",
      "Epoch[80/100], Batch[501/600], batch_loss:134507.375000\n",
      "======>epoch:80,\t epoch_average_batch_loss:134259.597318============ \n",
      "\n",
      "Epoch[81/100], Batch[1/600], batch_loss:134665.843750\n",
      "Epoch[81/100], Batch[101/600], batch_loss:134357.984375\n",
      "Epoch[81/100], Batch[201/600], batch_loss:134687.375000\n",
      "Epoch[81/100], Batch[301/600], batch_loss:133886.796875\n",
      "Epoch[81/100], Batch[401/600], batch_loss:134005.750000\n",
      "Epoch[81/100], Batch[501/600], batch_loss:133841.734375\n",
      "======>epoch:81,\t epoch_average_batch_loss:134270.587109============ \n",
      "\n",
      "save: ./img/hcvae/hepoch81.png \n",
      "\n",
      "Epoch[82/100], Batch[1/600], batch_loss:134456.750000\n",
      "Epoch[82/100], Batch[101/600], batch_loss:134259.250000\n",
      "Epoch[82/100], Batch[201/600], batch_loss:134126.062500\n",
      "Epoch[82/100], Batch[301/600], batch_loss:133962.406250\n",
      "Epoch[82/100], Batch[401/600], batch_loss:134174.656250\n",
      "Epoch[82/100], Batch[501/600], batch_loss:133806.843750\n",
      "======>epoch:82,\t epoch_average_batch_loss:134246.784062============ \n",
      "\n",
      "Epoch[83/100], Batch[1/600], batch_loss:133772.609375\n",
      "Epoch[83/100], Batch[101/600], batch_loss:134227.359375\n",
      "Epoch[83/100], Batch[201/600], batch_loss:134678.125000\n",
      "Epoch[83/100], Batch[301/600], batch_loss:134199.687500\n",
      "Epoch[83/100], Batch[401/600], batch_loss:134306.078125\n",
      "Epoch[83/100], Batch[501/600], batch_loss:133868.218750\n",
      "======>epoch:83,\t epoch_average_batch_loss:134296.396094============ \n",
      "\n",
      "Epoch[84/100], Batch[1/600], batch_loss:134485.343750\n",
      "Epoch[84/100], Batch[101/600], batch_loss:134441.781250\n",
      "Epoch[84/100], Batch[201/600], batch_loss:134476.593750\n",
      "Epoch[84/100], Batch[301/600], batch_loss:134211.687500\n",
      "Epoch[84/100], Batch[401/600], batch_loss:133341.718750\n",
      "Epoch[84/100], Batch[501/600], batch_loss:134293.812500\n",
      "======>epoch:84,\t epoch_average_batch_loss:134208.460729============ \n",
      "\n",
      "Epoch[85/100], Batch[1/600], batch_loss:134620.609375\n",
      "Epoch[85/100], Batch[101/600], batch_loss:134882.453125\n",
      "Epoch[85/100], Batch[201/600], batch_loss:134003.156250\n",
      "Epoch[85/100], Batch[301/600], batch_loss:134181.765625\n",
      "Epoch[85/100], Batch[401/600], batch_loss:134153.906250\n",
      "Epoch[85/100], Batch[501/600], batch_loss:133890.625000\n",
      "======>epoch:85,\t epoch_average_batch_loss:134211.762057============ \n",
      "\n",
      "Epoch[86/100], Batch[1/600], batch_loss:133899.093750\n",
      "Epoch[86/100], Batch[101/600], batch_loss:133867.265625\n",
      "Epoch[86/100], Batch[201/600], batch_loss:134343.281250\n",
      "Epoch[86/100], Batch[301/600], batch_loss:134696.031250\n",
      "Epoch[86/100], Batch[401/600], batch_loss:134261.234375\n",
      "Epoch[86/100], Batch[501/600], batch_loss:133970.125000\n",
      "======>epoch:86,\t epoch_average_batch_loss:134208.355104============ \n",
      "\n",
      "Epoch[87/100], Batch[1/600], batch_loss:133622.296875\n",
      "Epoch[87/100], Batch[101/600], batch_loss:134941.078125\n",
      "Epoch[87/100], Batch[201/600], batch_loss:133727.078125\n",
      "Epoch[87/100], Batch[301/600], batch_loss:133951.515625\n",
      "Epoch[87/100], Batch[401/600], batch_loss:134618.500000\n",
      "Epoch[87/100], Batch[501/600], batch_loss:134177.328125\n",
      "======>epoch:87,\t epoch_average_batch_loss:134179.130182============ \n",
      "\n",
      "Epoch[88/100], Batch[1/600], batch_loss:134221.750000\n",
      "Epoch[88/100], Batch[101/600], batch_loss:133991.250000\n",
      "Epoch[88/100], Batch[201/600], batch_loss:134481.671875\n",
      "Epoch[88/100], Batch[301/600], batch_loss:134524.484375\n",
      "Epoch[88/100], Batch[401/600], batch_loss:134090.656250\n",
      "Epoch[88/100], Batch[501/600], batch_loss:134632.421875\n",
      "======>epoch:88,\t epoch_average_batch_loss:134176.809740============ \n",
      "\n",
      "Epoch[89/100], Batch[1/600], batch_loss:134502.031250\n",
      "Epoch[89/100], Batch[101/600], batch_loss:133790.875000\n",
      "Epoch[89/100], Batch[201/600], batch_loss:133501.734375\n",
      "Epoch[89/100], Batch[301/600], batch_loss:134136.796875\n",
      "Epoch[89/100], Batch[401/600], batch_loss:134582.562500\n",
      "Epoch[89/100], Batch[501/600], batch_loss:134654.984375\n",
      "======>epoch:89,\t epoch_average_batch_loss:134245.934167============ \n",
      "\n",
      "Epoch[90/100], Batch[1/600], batch_loss:133865.437500\n",
      "Epoch[90/100], Batch[101/600], batch_loss:133721.437500\n",
      "Epoch[90/100], Batch[201/600], batch_loss:134155.546875\n",
      "Epoch[90/100], Batch[301/600], batch_loss:134622.546875\n",
      "Epoch[90/100], Batch[401/600], batch_loss:134387.921875\n",
      "Epoch[90/100], Batch[501/600], batch_loss:134356.218750\n",
      "======>epoch:90,\t epoch_average_batch_loss:134176.251875============ \n",
      "\n",
      "Epoch[91/100], Batch[1/600], batch_loss:134371.125000\n",
      "Epoch[91/100], Batch[101/600], batch_loss:133839.578125\n",
      "Epoch[91/100], Batch[201/600], batch_loss:133781.421875\n",
      "Epoch[91/100], Batch[301/600], batch_loss:134581.515625\n",
      "Epoch[91/100], Batch[401/600], batch_loss:133619.875000\n",
      "Epoch[91/100], Batch[501/600], batch_loss:134432.546875\n",
      "======>epoch:91,\t epoch_average_batch_loss:134210.022292============ \n",
      "\n",
      "save: ./img/hcvae/hepoch91.png \n",
      "\n",
      "Epoch[92/100], Batch[1/600], batch_loss:134415.281250\n",
      "Epoch[92/100], Batch[101/600], batch_loss:134422.250000\n",
      "Epoch[92/100], Batch[201/600], batch_loss:134128.953125\n",
      "Epoch[92/100], Batch[301/600], batch_loss:134124.406250\n",
      "Epoch[92/100], Batch[401/600], batch_loss:134176.015625\n",
      "Epoch[92/100], Batch[501/600], batch_loss:134420.734375\n",
      "======>epoch:92,\t epoch_average_batch_loss:134170.786042============ \n",
      "\n",
      "Epoch[93/100], Batch[1/600], batch_loss:133954.031250\n",
      "Epoch[93/100], Batch[101/600], batch_loss:134154.531250\n",
      "Epoch[93/100], Batch[201/600], batch_loss:134218.765625\n",
      "Epoch[93/100], Batch[301/600], batch_loss:133297.187500\n",
      "Epoch[93/100], Batch[401/600], batch_loss:134670.500000\n",
      "Epoch[93/100], Batch[501/600], batch_loss:133972.093750\n",
      "======>epoch:93,\t epoch_average_batch_loss:134163.523151============ \n",
      "\n",
      "Epoch[94/100], Batch[1/600], batch_loss:134214.140625\n",
      "Epoch[94/100], Batch[101/600], batch_loss:133736.375000\n",
      "Epoch[94/100], Batch[201/600], batch_loss:134831.906250\n",
      "Epoch[94/100], Batch[301/600], batch_loss:134547.468750\n",
      "Epoch[94/100], Batch[401/600], batch_loss:133834.718750\n",
      "Epoch[94/100], Batch[501/600], batch_loss:133994.312500\n",
      "======>epoch:94,\t epoch_average_batch_loss:134157.817500============ \n",
      "\n",
      "Epoch[95/100], Batch[1/600], batch_loss:134556.078125\n",
      "Epoch[95/100], Batch[101/600], batch_loss:133350.203125\n",
      "Epoch[95/100], Batch[201/600], batch_loss:134052.750000\n",
      "Epoch[95/100], Batch[301/600], batch_loss:133873.921875\n",
      "Epoch[95/100], Batch[401/600], batch_loss:134003.031250\n",
      "Epoch[95/100], Batch[501/600], batch_loss:134006.875000\n",
      "======>epoch:95,\t epoch_average_batch_loss:134160.239870============ \n",
      "\n",
      "Epoch[96/100], Batch[1/600], batch_loss:134054.125000\n",
      "Epoch[96/100], Batch[101/600], batch_loss:133994.187500\n",
      "Epoch[96/100], Batch[201/600], batch_loss:134140.843750\n",
      "Epoch[96/100], Batch[301/600], batch_loss:134403.890625\n",
      "Epoch[96/100], Batch[401/600], batch_loss:134624.562500\n",
      "Epoch[96/100], Batch[501/600], batch_loss:133486.562500\n",
      "======>epoch:96,\t epoch_average_batch_loss:134141.055833============ \n",
      "\n",
      "Epoch[97/100], Batch[1/600], batch_loss:134276.453125\n",
      "Epoch[97/100], Batch[101/600], batch_loss:134201.812500\n",
      "Epoch[97/100], Batch[201/600], batch_loss:133624.390625\n",
      "Epoch[97/100], Batch[301/600], batch_loss:134181.390625\n",
      "Epoch[97/100], Batch[401/600], batch_loss:134309.718750\n",
      "Epoch[97/100], Batch[501/600], batch_loss:133802.781250\n",
      "======>epoch:97,\t epoch_average_batch_loss:134133.669245============ \n",
      "\n",
      "Epoch[98/100], Batch[1/600], batch_loss:133543.718750\n",
      "Epoch[98/100], Batch[101/600], batch_loss:134348.640625\n",
      "Epoch[98/100], Batch[201/600], batch_loss:133737.500000\n",
      "Epoch[98/100], Batch[301/600], batch_loss:133831.593750\n",
      "Epoch[98/100], Batch[401/600], batch_loss:134105.218750\n",
      "Epoch[98/100], Batch[501/600], batch_loss:134180.031250\n",
      "======>epoch:98,\t epoch_average_batch_loss:134168.718620============ \n",
      "\n",
      "Epoch[99/100], Batch[1/600], batch_loss:134237.218750\n",
      "Epoch[99/100], Batch[101/600], batch_loss:133943.968750\n",
      "Epoch[99/100], Batch[201/600], batch_loss:133633.734375\n",
      "Epoch[99/100], Batch[301/600], batch_loss:134144.046875\n",
      "Epoch[99/100], Batch[401/600], batch_loss:134190.000000\n",
      "Epoch[99/100], Batch[501/600], batch_loss:134040.031250\n",
      "======>epoch:99,\t epoch_average_batch_loss:134140.128464============ \n",
      "\n",
      "Epoch[100/100], Batch[1/600], batch_loss:134152.156250\n",
      "Epoch[100/100], Batch[101/600], batch_loss:133936.015625\n",
      "Epoch[100/100], Batch[201/600], batch_loss:133782.218750\n",
      "Epoch[100/100], Batch[301/600], batch_loss:134240.843750\n",
      "Epoch[100/100], Batch[401/600], batch_loss:134296.437500\n",
      "Epoch[100/100], Batch[501/600], batch_loss:134451.218750\n",
      "======>epoch:100,\t epoch_average_batch_loss:134099.252917============ \n",
      "\n",
      "save raw image:./img/cvae/raw/png \n",
      "\n",
      "save model..........\n"
     ]
    }
   ],
   "source": [
    "def test2():\n",
    "    epochs = 100\n",
    "    batch_size = 100\n",
    "\n",
    "    recon = None\n",
    "    img = None\n",
    "\n",
    "    utils.make_dir(\"./img/cvae\")\n",
    "    utils.make_dir(\"./model_weights/cvae\")\n",
    "\n",
    "    train_data = torchvision.datasets.MNIST(\n",
    "        root='./mnist',\n",
    "        train=True,\n",
    "        transform=torchvision.transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    data_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "    cvae = HCVAE2(feature_size=784, class_size=10, latent_size=10)\n",
    "    #cvae = CVAE(feature_size=784, class_size=10, latent_size=10)\n",
    "\n",
    "    optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        train_loss = 0\n",
    "        i = 0\n",
    "        for batch_id, data in enumerate(data_loader):\n",
    "            img, label = data\n",
    "            inputs = img.reshape(img.shape[0], -1)\n",
    "            y = utils.to_one_hot(label.reshape(-1, 1), num_class=10)\n",
    "            #recon, mu, log_std = cvae(inputs, y)\n",
    "            #loss = cvae.loss_function(recon, inputs, mu, log_std)\n",
    "            recon, z,en_mu, de_mu, log_std = cvae.forward(inputs, y)\n",
    "            loss = cvae.loss_function(recon, z, inputs, en_mu, de_mu, log_std)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            i += 1\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"Epoch[{}/{}], Batch[{}/{}], batch_loss:{:.6f}\".format(\n",
    "                    epoch+1, epochs, batch_id+1, len(data_loader), loss.item()))\n",
    "\n",
    "        print(\"======>epoch:{},\\t epoch_average_batch_loss:{:.6f}============\".format(\n",
    "            epoch+1, train_loss/i), \"\\n\")\n",
    "\n",
    "        # save imgs\n",
    "        if epoch % 10 == 0:\n",
    "            # 查看图像\n",
    "            imgs = utils.to_img(recon[0].detach())\n",
    "            path = \"./img/hcvae/hepoch{}.png\".format(epoch+1)\n",
    "            torchvision.utils.save_image(imgs, path, nrow=10)\n",
    "            print(\"save:\", path, \"\\n\")\n",
    "\n",
    "    torchvision.utils.save_image(img, \"./img/cvae/raw.png\", nrow=10)\n",
    "    print(\"save raw image:./img/cvae/raw/png\", \"\\n\")\n",
    "\n",
    "    # save val model\n",
    "    utils.save_model(cvae, \"./model_weights/cvae/cvae_weights.pth\")\n",
    "\n",
    "test2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 创建包含10000个一维数组的张量\n",
    "num_samples = 10000\n",
    "input_size = 20\n",
    "data = torch.randn(num_samples, input_size)\n",
    "\n",
    "# 创建神经网络实例\n",
    "net = HCVAE2(feature_size=20, class_size=8, latent_size=4)\n",
    "\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()  # 均方误差损失\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# 迭代训练神经网络\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # 正向传播\n",
    "    outputs = net.forward(data)\n",
    "    \n",
    "    # 生成一个示例的目标数据，这里假设目标数据也是随机生成的\n",
    "    targets = torch.randn(num_samples, 1)\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # 反向传播和优化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 打印损失\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# 训练完成后，您可以使用神经网络进行预测\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "240bc028caeb8b02ff80d8aedfc61caf7a0e4db2770780d40c5b717508bae340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
